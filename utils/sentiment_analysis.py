"""
ê°ì„±ë¶„ì„ ëª¨ë“ˆ
- í•œêµ­ì–´ ê°ì„± ì‚¬ì „ ê¸°ë°˜ ë¶„ì„
- ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ë¶„ë¥˜
- ì ìˆ˜í™” (0-100)
"""

from typing import List, Dict, Tuple
from collections import Counter
import re

# ============================================
# í•œêµ­ì–´ ê°ì„± ì‚¬ì „ 
# ============================================

POSITIVE_WORDS = {
    # ê¸°ë³¸ ê¸ì •ì–´
    "ì¢‹ë‹¤", "ì¢‹ì•„", "ì¢‹ì€", "ì¢‹ë„¤", "ì¢‹ì•„ìš”", "ìµœê³ ", "ëŒ€ë°•", "ê°ì‚¬", "ê°ë™",
    "ì¬ë°Œ", "ì¬ë¯¸", "ì¬ë°Œë‹¤", "ìœ ìµ", "ë„ì›€", "ìœ ìš©", "í›Œë¥­", "ë©‹ì§€", "êµ¿",
    "ì™„ë²½", "ì‚¬ë‘", "ì¶”ì²œ", "ê°•ì¶”", "ì¸ì •", "ğŸ‘", "â¤ï¸", "ğŸ’•", "ğŸ˜Š", "ğŸ”¥",
    
    # ì¶”ê°€ ê¸ì •ì–´
    "ìµì˜¤", "êµ¿êµ¿", "ì§±", "ê°œì¢‹", "ë ˆì „ë“œ", "ê°“", "ì‹ ì˜í•œìˆ˜", "í•µì¸ì‹¸",
    "ê¿€íŒ", "ì•Œì°¨", "ì•Œì°¬", "ì •ì£¼í–‰", "ì¡´ê²½", "ë°°ìš°", "ë°°ì›Œ", "ë°°ì› ",
    "ê³ ë§™", "ê°ì‚¬í•©ë‹ˆë‹¤", "ì •ì„±", "ì„¼ìŠ¤", "ì›ƒ", "ì›ƒê²¨", "ì›ƒê¸´", "ã…‹ã…‹",
    "ê°ë™ì ", "ë”°ëœ»", "í–‰ë³µ", "íë§", "ìœ„ë¡œ", "ê³µê°", "ìš¸ì»¥", "ë­‰í´",
    "í”„ë¡œ", "ì „ë¬¸", "ì‹¤ë ¥", "ê¼¼ê¼¼", "ì¹œì ˆ", "ê¹”ë”", "ê¹¨ë—", "ì •í™•"
}

NEGATIVE_WORDS = {
    # ê¸°ë³¸ ë¶€ì •ì–´
    "ë³„ë¡œ", "ì‹«ë‹¤", "ì‹«ì–´", "ìµœì•…", "ì§€ë£¨", "ì§œì¦", "ì‹¤ë§", "ê±°ì§“", "ê´‘ê³ ",
    "ì‹œê°„ë‚­ë¹„", "ëˆì•„ê¹Œ", "í›„íšŒ", "ì†ì•˜", "ê³¼ëŒ€ê´‘ê³ ", "ğŸ‘", "ğŸ˜¡", "ğŸ˜¢",
    
    # ì¶”ê°€ ë¶€ì •ì–´
    "ë…¸ì¼", "ë³„ë¡œì•¼", "ì•ˆì¢‹", "êµ¬ë ¤", "í˜•í¸ì—†", "ì•„ì‰½", "ë³„ë¡œë„¤", "ì‹¤ë§ì´ì•¼",
    "ë»”", "ë»”í•˜", "ì‹ìƒ", "ì¬ë¯¸ì—†", "ì§€ë£¨í•´", "ë£¨ì¦ˆ", "ë£¨ì¦ˆí•´", "ì¥í™©",
    "ì´í•´ì•ˆ", "ì´í•´ë¶ˆê°€", "ì–µì§€", "ì˜¤ê·¸ë¼", "ë¯¼ë§", "ë¶€ë„", "ì°½í”¼",
    "ë¶ˆì¹œì ˆ", "ë¶ˆí¸", "ë¶ˆë§Œ", "ì§œì¦ë‚˜", "í™”ë‚˜", "ì—´ë°›", "ì–´ì´ì—†", "í™©ë‹¹",
    "ë¹„ì¶”", "ë¹„ì¶”ì²œ", "ì¶”ì²œì•ˆ", "ë§ë¦¬", "ì‚¬ì§€ë§ˆ", "ë³´ì§€ë§ˆ", "ê±°ë¥´", "íŒ¨ìŠ¤"
}

NEUTRAL_INDICATORS = {
    "ê·¸ëƒ¥", "ë³´í†µ", "ê·¸ì €ê·¸ë˜", "ë¬´ë‚œ", "í‰ë²”", "soso", "ì˜ì˜", "ê¸€ì„", "ëª¨ë¥´ê² "
}

# ê°•ì¡° í‘œí˜„
INTENSIFIERS = {
    "ì§„ì§œ", "ì •ë§", "ë„ˆë¬´", "ì—„ì²­", "ì™„ì „", "ê°œ", "í•µ", "ì¡´", "ë§¤ìš°", "ë¬´ì²™",
    "ì•„ì£¼", "ì •ë§ë¡œ", "ì§„ì‹¬", "ë ˆì•Œ", "ã„¹ã…‡", "ì˜¤ì§€ê²Œ"
}

# ë¶€ì • í‘œí˜„ (ë¶€ì •ì˜ ë¶€ì • = ê¸ì •)
NEGATIONS = {
    "ì•ˆ", "ëª»", "ì ˆëŒ€", "ì „í˜€", "ì—†", "ì•„ë‹ˆ"
}

# ============================================
# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
# ============================================

def clean_text(text: str) -> str:
    """ëŒ“ê¸€ í…ìŠ¤íŠ¸ ì •ì œ"""
    # ì´ëª¨ì§€ ì œê±° (ë‹¨, ê°ì„± ë¶„ì„ìš© ì´ëª¨ì§€ëŠ” ë¯¸ë¦¬ ì¶”ì¶œ)
    text = re.sub(r'http\S+', '', text)  # URL ì œê±°
    text = re.sub(r'@\w+', '', text)  # ë©˜ì…˜ ì œê±°
    text = re.sub(r'#\w+', '', text)  # í•´ì‹œíƒœê·¸ ì œê±°
    text = re.sub(r'[^\w\sê°€-í£ã„±-ã…ã…-ã…£!?.,]', '', text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = ' '.join(text.split())  # ê³µë°± ì •ë¦¬
    return text.lower()

def extract_emojis(text: str) -> List[str]:
    """ì´ëª¨ì§€ ì¶”ì¶œ"""
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # ê°ì •
        "\U0001F300-\U0001F5FF"  # ì‹¬ë³¼
        "\U0001F680-\U0001F6FF"  # êµí†µ
        "\U0001F1E0-\U0001F1FF"  # êµ­ê¸°
        "\U00002702-\U000027B0"
        "\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE
    )
    return emoji_pattern.findall(text)

# ============================================
# ê°ì„± ë¶„ì„ í•µì‹¬ í•¨ìˆ˜
# ============================================

def analyze_sentiment_advanced(text: str) -> Tuple[str, float]:
    """
    ê³ ë„í™”ëœ ê°ì„±ë¶„ì„

    Returns:
        (ê°ì„±, í™•ì‹ ë„) - ê°ì„±: "positive", "nagative", "neutral", í™•ì‹ ë„: 0-1
    """
    original_text = text
    text = clean_text(text)
    emojis = extract_emojis(original_text)

    # 1. ì´ëª¨ì§€ ê¸°ë°˜ ê°ì„± íŒë‹¨
    emoji_score = 0
    for emoji in emojis:
        if emoji in {"ğŸ‘", "â¤ï¸", "ğŸ’•", "ğŸ˜Š", "ğŸ”¥", "ğŸ˜", "ğŸ¥°", "ğŸ˜", "ğŸ˜„","ğŸ¥µ"}:
            emoji_score += 2
        elif emoji in {"ğŸ‘", "ğŸ˜¡", "ğŸ˜¢", "ğŸ˜­", "ğŸ’”", "ğŸ˜", "ğŸ˜ "}:
            emoji_score -= 2
        
    # 2. ë‹¨ì–´ ê¸°ë°˜ ê°ì„± ì ìˆ˜
    words = text.split()
    pos_count = 0
    neg_count = 0

    # ê°•ì¡°ì–´ ê°ì§€
    has_intensifier = any(word in text for word in INTENSIFIERS)
    intensity_multiplier = 1.5 if has_intensifier else 1.0

    # ë¶€ì •ì–´ ê°ì§€
    negation_count = sum(1 for word in NEGATIONS if word in text)

    for word in words:
        if any(pos in word for pos in POSITIVE_WORDS):
            pos_count += 1
        if any(neg in word for neg in NEGATIVE_WORDS):
            neg_count += 1
    
    # ë¶€ì •ì–´ê°€ í™€ìˆ˜ê°œë©´ ê°ì„± ë°˜ì „
    if negation_count % 2 == 1:
        pos_count, neg_count = neg_count, pos_count

     # ê°•ì¡°ì–´ ì ìš©
    pos_score = pos_count * intensity_multiplier
    neg_score = neg_count * intensity_multiplier

    # 3. ìµœì¢… ì ìˆ˜ ê³„ì‚°
    total_score = (pos_score - neg_score) + emoji_score

     # 4. ì¤‘ë¦½ íŒë‹¨
    neutral_detected = any(neutral in text for neutral in NEUTRAL_INDICATORS)

    # 5. ê°ì„± ê²°ì •
    if neutral_detected or abs(total_score) < 1:
        return "neutral", 0.5
    elif total_score > 0:
        confidence = min(total_score / 5, 1.0)  # ìµœëŒ€ 1.0
        return "positive", confidence
    else:
        confidence = min(abs(total_score) / 5, 1.0)
        return "negative", confidence

def analyze_comments_batch(comments: List[str]) -> Dict[str, any]:
    """
    ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¼ê´„ ë¶„ì„

    Returns:
        {
            "positive": ê¸ì • ëŒ“ê¸€ ìˆ˜,
            "negative": ë¶€ì • ëŒ“ê¸€ ìˆ˜,
            "neutral": ì¤‘ë¦½ ëŒ“ê¸€ ìˆ˜,
            "positive_ratio": ê¸ì • ë¹„ìœ¨ (%),
            "sentiment_score": ìµœì¢… ê°ì„± ì ìˆ˜ (0-100),
            "total_comments": ì „ì²´ ëŒ“ê¸€ ìˆ˜,
            "examples": {
                "positive": [...],
                "negative": [...]
            }
        }
    """
    if not comments:
        return {
            "positive": 0,
            "negative": 0,
            "neutral": 0,
            "positive_ratio": 0.0,
            "negative_ratio": 0.0,
            "neutral_ratio": 0.0,
            "sentiment_score": 50.0,
            "total_comments": 0,
            "examples": {"positive": [], "negative": []}
        }
    
    results = {"positive": [], "negative": [], "neutral": []}

    for comment in comments: 
        sentiment, confidence = analyze_sentiment_advanced(comment)
        results[sentiment].append(comment, confidence)

    pos_count = len(results["positive"])
    neg_count = len(results["negative"])
    neu_count = len(results["neutral"])
    total = len(comments)

    # ë¹„ìœ¨ ê³„ì‚°
    pos_ratio = (pos_count / total) * 100
    neg_ratio = (neg_count / total) * 100
    neu_ratio = (neu_count / total) * 100

    # ê°ì„± ì ìˆ˜ (0-100)
    # ê¸ì • ë§ì„ìˆ˜ë¡ 100ì— ê°€ê¹Œì›€, ë¶€ì • ë§ì„ìˆ˜ë¡ 0ì— ê°€ê¹Œì›€
    sentiment_score = (pos_ratio - neg_ratio + 100) / 2
    sentiment_score = max(0, min(100, sentiment_score)) # 0-100 ë²”ìœ„ë¡œ ì œí•œ

    # ëŒ€í‘œ ì˜ˆì‹œ ì¶”ì¶œ (í™•ì‹ ë„ ë†’ì€ ìˆœ)
    pos_examples = sorted(results["positive"], key=lambda x: x[1], reverse=True)[:5]
    neg_examples = sorted(results["negative"], key=lambda x: x[1], reverse=True)[:5]

    return {
        "positive": pos_count,
        "negative": neg_count,
        "neutral": neu_count,
        "positive_ratio": round(pos_ratio, 2),
        "negative_ratio": round(neg_ratio, 2),
        "neutral_ratio": round(neu_ratio, 2),
        "sentiment_score": round(sentiment_score, 2),
        "total_comments": total,
        "examples": {
            "positive": [text[:100] for text, _ in pos_examples],
            "negative": [text[:100] for text, _ in neg_examples]
        }
    }

# ============================================
# í‚¤ì›Œë“œ ì¶”ì¶œ 
# ============================================

def extract_keywords_improved(comments: List[str], top_k: int = 20) -> List[Dict[str, any]]:
    """
    ê°œì„ ëœ í‚¤ì›Œë“œ ì¶”ì¶œ
    - ë¶ˆìš©ì–´ ì œê±°
    - ëª…ì‚¬ ìš°ì„  ì¶”ì¶œ
    - ê°ì„± í‚¤ì›Œë“œ ë¶„ë¦¬
    """
    # í•œêµ­ì–´ ë¶ˆìš©ì–´
    STOPWORDS = {
        "ìˆ", "ì—†", "í•˜", "ë˜", "ì´", "ê·¸", "ì €", "ê²ƒ", "ìˆ˜", "ë“±", "ë°", "ì œ", "ì•½",
        "ì¦‰", "ë°", "ë˜í•œ", "ë˜ëŠ”", "ì˜", "ê°€", "ì´", "ì€", "ëŠ”", "ì„", "ë¥¼", "ì—",
        "ì—ì„œ", "ìœ¼ë¡œ", "ë¡œ", "ê³¼", "ì™€", "í•œ", "í• ", "í•˜ë‹¤", "ë˜ë‹¤"
    }
    
    all_words = []
    for comment in comments:
        clean = clean_text(comment)
        words = [w for w in clean.split() if len(w) > 1 and w not in STOPWORDS]
        all_words.extend(words)
    
    # ë¹ˆë„ìˆ˜ ê³„ì‚°
    word_counts = Counter(all_words)
    
    # ê°ì„± ë‹¨ì–´ ì œì™¸í•˜ê³  ìˆœìˆ˜ í‚¤ì›Œë“œë§Œ
    keywords = []
    for word, count in word_counts.most_common(top_k * 2):
        # ê°ì„± ë‹¨ì–´ê°€ ì•„ë‹Œ ê²ƒë§Œ ì¶”ì¶œ
        if not any(w in word for w in POSITIVE_WORDS | NEGATIVE_WORDS):
            keywords.append({
                "keyword": word,
                "count": count,
                "frequency": round(count / len(comments) * 100, 2)
            })
            
            if len(keywords) >= top_k:
                break
    
    return keywords
"""
ê°ì„±ë¶„ì„ ëª¨ë“ˆ
- KoBERT ê¸°ë°˜ ê°ì„±ë¶„ì„ (ì„ íƒì , lazy loading)
- ì‚¬ì „ ê¸°ë°˜ ê°ì„±ë¶„ì„ (fallback)
- ê¸ì •/ë¶€ì •/ì¤‘ë¦½ 3-class ë¶„ë¥˜
"""

from typing import List, Dict, Tuple, Optional
from collections import Counter
import re

# ============================================
# KoBERT ëª¨ë¸ (Lazy Loading)
# ============================================

MODEL_NAME = "monologg/kobert"
_tokenizer = None
_model = None
_device = None
_kobert_available = False

SENTIMENT_LABELS = {
    0: "negative",
    1: "neutral", 
    2: "positive"
}

def _load_kobert_model():
    """KoBERT ëª¨ë¸ì„ lazy loadingìœ¼ë¡œ ë¡œë“œ"""
    global _tokenizer, _model, _device, _kobert_available
    
    if _kobert_available:
        return True
    
    try:
        import torch
        from transformers import AutoTokenizer, AutoModelForSequenceClassification
        
        _device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"[KoBERT] ëª¨ë¸ ë¡œë”© ì‹œë„ ì¤‘... (Device: {_device})")
        
        _tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        _model = AutoModelForSequenceClassification.from_pretrained(
            MODEL_NAME,
            num_labels=3
        ).to(_device)
        _model.eval()
        
        _kobert_available = True
        print("[KoBERT] ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        return True
    except Exception as e:
        print(f"[KoBERT] ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨, ì‚¬ì „ ê¸°ë°˜ ë¶„ì„ ì‚¬ìš©: {e}")
        _kobert_available = False
        return False

# ============================================
# ì‚¬ì „ ê¸°ë°˜ ê°ì„± ë¶„ì„ (Fallback)
# ============================================

POSITIVE_WORDS = {
    "ì¢‹ë‹¤", "ì¢‹ì•„", "ì¢‹ì€", "ì¢‹ë„¤", "ì¢‹ì•„ìš”", "ìµœê³ ", "ëŒ€ë°•", "ê°ì‚¬", "ê°ë™",
    "ì¬ë°Œ", "ì¬ë¯¸", "ì¬ë°Œë‹¤", "ìœ ìµ", "ë„ì›€", "ìœ ìš©", "í›Œë¥­", "ë©‹ì§€", "êµ¿",
    "ì™„ë²½", "ì‚¬ë‘", "ì¶”ì²œ", "ê°•ì¶”", "ì¸ì •", "ğŸ‘", "â¤ï¸", "ğŸ’•", "ğŸ˜Š", "ğŸ”¥",
    "ìµì˜¤", "êµ¿êµ¿", "ì§±", "ê°œì¢‹", "ë ˆì „ë“œ", "ê°“", "ì‹ ì˜í•œìˆ˜", "í•µì¸ì‹¸",
    "ê¿€íŒ", "ì•Œì°¨", "ì•Œì°¬", "ì •ì£¼í–‰", "ì¡´ê²½", "ë°°ìš°", "ë°°ì›Œ", "ë°°ì› ",
    "ê³ ë§™", "ê°ì‚¬í•©ë‹ˆë‹¤", "ì •ì„±", "ì„¼ìŠ¤", "ì›ƒ", "ì›ƒê²¨", "ì›ƒê¸´", "ã…‹ã…‹",
    "ê°ë™ì ", "ë”°ëœ»", "í–‰ë³µ", "íë§", "ìœ„ë¡œ", "ê³µê°", "ìš¸ì»¥", "ë­‰í´",
    "í”„ë¡œ", "ì „ë¬¸", "ì‹¤ë ¥", "ê¼¼ê¼¼", "ì¹œì ˆ", "ê¹”ë”", "ê¹¨ë—", "ì •í™•"
}

NEGATIVE_WORDS = {
    "ë³„ë¡œ", "ì‹«ë‹¤", "ì‹«ì–´", "ìµœì•…", "ì§€ë£¨", "ì§œì¦", "ì‹¤ë§", "ê±°ì§“", "ê´‘ê³ ",
    "ì‹œê°„ë‚­ë¹„", "ëˆì•„ê¹Œ", "í›„íšŒ", "ì†ì•˜", "ê³¼ëŒ€ê´‘ê³ ", "ğŸ‘", "ğŸ˜¡", "ğŸ˜¢",
    "ë…¸ì¼", "ë³„ë¡œì•¼", "ì•ˆì¢‹", "êµ¬ë ¤", "í˜•í¸ì—†", "ì•„ì‰½", "ë³„ë¡œë„¤", "ì‹¤ë§ì´ì•¼",
    "ë»”", "ë»”í•˜", "ì‹ìƒ", "ì¬ë¯¸ì—†", "ì§€ë£¨í•´", "ë£¨ì¦ˆ", "ë£¨ì¦ˆí•´", "ì¥í™©",
    "ì´í•´ì•ˆ", "ì´í•´ë¶ˆê°€", "ì–µì§€", "ì˜¤ê·¸ë¼", "ë¯¼ë§", "ë¶€ë„", "ì°½í”¼",
    "ë¶ˆì¹œì ˆ", "ë¶ˆí¸", "ë¶ˆë§Œ", "ì§œì¦ë‚˜", "í™”ë‚˜", "ì—´ë°›", "ì–´ì´ì—†", "í™©ë‹¹",
    "ë¹„ì¶”", "ë¹„ì¶”ì²œ", "ì¶”ì²œì•ˆ", "ë§ë¦¬", "ì‚¬ì§€ë§ˆ", "ë³´ì§€ë§ˆ", "ê±°ë¥´", "íŒ¨ìŠ¤"
}

NEUTRAL_INDICATORS = {
    "ê·¸ëƒ¥", "ë³´í†µ", "ê·¸ì €ê·¸ë˜", "ë¬´ë‚œ", "í‰ë²”", "soso", "ì˜ì˜", "ê¸€ì„", "ëª¨ë¥´ê² "
}

INTENSIFIERS = {
    "ì§„ì§œ", "ì •ë§", "ë„ˆë¬´", "ì—„ì²­", "ì™„ì „", "ê°œ", "í•µ", "ì¡´", "ë§¤ìš°", "ë¬´ì²™",
    "ì•„ì£¼", "ì •ë§ë¡œ", "ì§„ì‹¬", "ë ˆì•Œ", "ã„¹ã…‡", "ì˜¤ì§€ê²Œ"
}

NEGATIONS = {
    "ì•ˆ", "ëª»", "ì ˆëŒ€", "ì „í˜€", "ì—†", "ì•„ë‹ˆ"
}

# ============================================
# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
# ============================================

def clean_text_for_bert(text: str) -> str:
    """BERT ì…ë ¥ìš© í…ìŠ¤íŠ¸ ì •ì œ"""
    # URL, ë©˜ì…˜ ì œê±°
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'@\w+', '', text)
    text = re.sub(r'#\w+', '', text)
    
    # ê³¼ë„í•œ ë°˜ë³µ ë¬¸ì ì œê±° (ã…‹ã…‹ã…‹ã…‹ â†’ ã…‹ã…‹)
    text = re.sub(r'(.)\1{2,}', r'\1\1', text)
    
    # ê³µë°± ì •ë¦¬
    text = ' '.join(text.split())
    
    return text.strip()

def analyze_sentiment_dict(text: str) -> Tuple[str, float]:
    """
    ì‚¬ì „ ê¸°ë°˜ ê°ì„± ë¶„ì„ (fallback)
    """
    original_text = text
    text = clean_text_for_bert(text).lower()
    
    # ì´ëª¨ì§€ ì¶”ì¶œ
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"
        "\U0001F300-\U0001F5FF"
        "\U0001F680-\U0001F6FF"
        "\U0001F1E0-\U0001F1FF"
        "\U00002702-\U000027B0"
        "\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE
    )
    emojis = emoji_pattern.findall(original_text)
    
    # ì´ëª¨ì§€ ê¸°ë°˜ ì ìˆ˜
    emoji_score = 0
    for emoji in emojis:
        if emoji in {"ğŸ‘", "â¤ï¸", "ğŸ’•", "ğŸ˜Š", "ğŸ”¥", "ğŸ˜", "ğŸ¥°", "ğŸ˜", "ğŸ˜„", "ğŸ¥µ"}:
            emoji_score += 2
        elif emoji in {"ğŸ‘", "ğŸ˜¡", "ğŸ˜¢", "ğŸ˜­", "ğŸ’”", "ğŸ˜", "ğŸ˜ "}:
            emoji_score -= 2
    
    # ë‹¨ì–´ ê¸°ë°˜ ê°ì„± ì ìˆ˜
    words = text.split()
    pos_count = 0
    neg_count = 0
    
    has_intensifier = any(word in text for word in INTENSIFIERS)
    intensity_multiplier = 1.5 if has_intensifier else 1.0
    negation_count = sum(1 for word in NEGATIONS if word in text)
    
    for word in words:
        if any(pos in word for pos in POSITIVE_WORDS):
            pos_count += 1
        if any(neg in word for neg in NEGATIVE_WORDS):
            neg_count += 1
    
    # ë¶€ì •ì–´ í™€ìˆ˜ê°œë©´ ê°ì„± ë°˜ì „
    if negation_count % 2 == 1:
        pos_count, neg_count = neg_count, pos_count
    
    pos_score = pos_count * intensity_multiplier
    neg_score = neg_count * intensity_multiplier
    total_score = (pos_score - neg_score) + emoji_score
    
    neutral_detected = any(neutral in text for neutral in NEUTRAL_INDICATORS)
    
    if neutral_detected or abs(total_score) < 1:
        return "neutral", 0.5
    elif total_score > 0:
        confidence = min(total_score / 5, 1.0)
        return "positive", confidence
    else:
        confidence = min(abs(total_score) / 5, 1.0)
        return "negative", confidence

# ============================================
# KoBERT ê°ì„± ë¶„ì„ í•µì‹¬ í•¨ìˆ˜
# ============================================

def analyze_sentiment_kobert(text: str) -> Tuple[str, float]:
    """
    KoBERT ê¸°ë°˜ ê°ì„±ë¶„ì„ (ê°€ëŠ¥í•œ ê²½ìš°), ì•„ë‹ˆë©´ ì‚¬ì „ ê¸°ë°˜ ë¶„ì„
    
    Args:
        text: ë¶„ì„í•  í…ìŠ¤íŠ¸
    
    Returns:
        (ê°ì„±, í™•ì‹ ë„) 
        - ê°ì„±: "positive", "negative", "neutral"
        - í™•ì‹ ë„: 0.0 ~ 1.0
    """
    # KoBERT ëª¨ë¸ ë¡œë“œ ì‹œë„
    if not _load_kobert_model():
        # KoBERT ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì‚¬ì „ ê¸°ë°˜ ë¶„ì„
        return analyze_sentiment_dict(text)
    
    # 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    cleaned_text = clean_text_for_bert(text)
    
    if not cleaned_text or len(cleaned_text) < 2:
        return "neutral", 0.5
    
    try:
        import torch
        
        # 2. í† í¬ë‚˜ì´ì§• (ìµœëŒ€ 512 í† í°)
        inputs = _tokenizer(
            cleaned_text,
            return_tensors="pt",
            max_length=512,
            padding=True,
            truncation=True
        ).to(_device)
        
        # 3. ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad():
            outputs = _model(**inputs)
            logits = outputs.logits
            
            # 4. Softmaxë¡œ í™•ë¥  ë³€í™˜
            probabilities = torch.softmax(logits, dim=1)[0]
            
            # 5. ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ê°ì„± ì„ íƒ
            predicted_class = torch.argmax(probabilities).item()
            confidence = probabilities[predicted_class].item()
            
            sentiment = SENTIMENT_LABELS[predicted_class]
            
        return sentiment, confidence
        
    except Exception as e:
        print(f"[KoBERT Error] {text[:50]}: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì‚¬ì „ ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ fallback
        return analyze_sentiment_dict(text)


def analyze_sentiment_kobert_batch(texts: List[str], batch_size: int = 32) -> List[Tuple[str, float]]:
    """
    ëŒ€ëŸ‰ í…ìŠ¤íŠ¸ ë°°ì¹˜ ì²˜ë¦¬ (ì†ë„ ìµœì í™”)
    
    Args:
        texts: ë¶„ì„í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        batch_size: ë°°ì¹˜ í¬ê¸°
    
    Returns:
        [(ê°ì„±, í™•ì‹ ë„), ...] ë¦¬ìŠ¤íŠ¸
    """
    # KoBERT ëª¨ë¸ ë¡œë“œ ì‹œë„
    if not _load_kobert_model():
        # KoBERT ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì‚¬ì „ ê¸°ë°˜ ë¶„ì„
        return [analyze_sentiment_dict(text) for text in texts]
    
    results = []
    
    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        cleaned_batch = [clean_text_for_bert(t) for t in batch]
        
        # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§
        valid_texts = [t for t in cleaned_batch if t and len(t) >= 2]
        
        if not valid_texts:
            results.extend([("neutral", 0.5)] * len(batch))
            continue
        
        try:
            import torch
            
            # ë°°ì¹˜ í† í¬ë‚˜ì´ì§•
            inputs = _tokenizer(
                valid_texts,
                return_tensors="pt",
                max_length=512,
                padding=True,
                truncation=True
            ).to(_device)
            
            # ë°°ì¹˜ ì¶”ë¡ 
            with torch.no_grad():
                outputs = _model(**inputs)
                logits = outputs.logits
                probabilities = torch.softmax(logits, dim=1)
                
                # ê° ìƒ˜í”Œì˜ ì˜ˆì¸¡
                predicted_classes = torch.argmax(probabilities, dim=1)
                confidences = torch.max(probabilities, dim=1).values
                
                for pred_class, conf in zip(predicted_classes, confidences):
                    sentiment = SENTIMENT_LABELS[pred_class.item()]
                    results.append((sentiment, conf.item()))
        
        except Exception as e:
            print(f"[KoBERT Batch Error] {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ ë°°ì¹˜ë¥¼ ì‚¬ì „ ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ fallback
            for text in batch:
                results.append(analyze_sentiment_dict(text))
    
    return results


def analyze_comments_batch_kobert(comments: List[str]) -> Dict[str, any]:
    """
    KoBERT ê¸°ë°˜ ëŒ“ê¸€ ì¼ê´„ ë¶„ì„
    
    Returns:
        {
            "positive": ê¸ì • ëŒ“ê¸€ ìˆ˜,
            "negative": ë¶€ì • ëŒ“ê¸€ ìˆ˜,
            "neutral": ì¤‘ë¦½ ëŒ“ê¸€ ìˆ˜,
            "positive_ratio": ê¸ì • ë¹„ìœ¨ (%),
            "sentiment_score": ìµœì¢… ê°ì„± ì ìˆ˜ (0-100),
            "total_comments": ì „ì²´ ëŒ“ê¸€ ìˆ˜,
            "examples": {...}
        }
    """
    if not comments:
        return {
            "positive": 0,
            "negative": 0,
            "neutral": 0,
            "positive_ratio": 0.0,
            "negative_ratio": 0.0,
            "neutral_ratio": 0.0,
            "sentiment_score": 50.0,
            "total_comments": 0,
            "examples": {"positive": [], "negative": []}
        }
    
    # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ê°ì„± ë¶„ì„
    method = "KoBERT" if _kobert_available else "ì‚¬ì „ê¸°ë°˜"
    print(f"[ê°ì„±ë¶„ì„ {method}] {len(comments)}ê°œ ëŒ“ê¸€ ë¶„ì„ ì¤‘...")
    sentiment_results = analyze_sentiment_kobert_batch(comments, batch_size=32)
    
    # ê²°ê³¼ ì§‘ê³„
    results = {"positive": [], "negative": [], "neutral": []}
    
    for comment, (sentiment, confidence) in zip(comments, sentiment_results):
        results[sentiment].append((comment, confidence))
    
    pos_count = len(results["positive"])
    neg_count = len(results["negative"])
    neu_count = len(results["neutral"])
    total = len(comments)
    
    # ë¹„ìœ¨ ê³„ì‚°
    pos_ratio = (pos_count / total) * 100
    neg_ratio = (neg_count / total) * 100
    neu_ratio = (neu_count / total) * 100
    
    # ê°ì„± ì ìˆ˜ (0-100)
    sentiment_score = (pos_ratio - neg_ratio + 100) / 2
    sentiment_score = max(0, min(100, sentiment_score))
    
    # ëŒ€í‘œ ì˜ˆì‹œ ì¶”ì¶œ (í™•ì‹ ë„ ë†’ì€ ìˆœ)
    pos_examples = sorted(results["positive"], key=lambda x: x[1], reverse=True)[:5]
    neg_examples = sorted(results["negative"], key=lambda x: x[1], reverse=True)[:5]
    
    return {
        "positive": pos_count,
        "negative": neg_count,
        "neutral": neu_count,
        "positive_ratio": round(pos_ratio, 2),
        "negative_ratio": round(neg_ratio, 2),
        "neutral_ratio": round(neu_ratio, 2),
        "sentiment_score": round(sentiment_score, 2),
        "total_comments": total,
        "examples": {
            "positive": [text[:100] for text, _ in pos_examples],
            "negative": [text[:100] for text, _ in neg_examples]
        },
        "model_info": {
            "model": MODEL_NAME if _kobert_available else "dict-based",
            "device": str(_device) if _kobert_available else "N/A",
            "batch_processing": True,
            "kobert_available": _kobert_available
        }
    }


# ============================================
# í‚¤ì›Œë“œ ì¶”ì¶œ (ê¸°ì¡´ ìœ ì§€)
# ============================================

def extract_keywords_improved(comments: List[str], top_k: int = 20) -> List[Dict[str, any]]:
    """ê°œì„ ëœ í‚¤ì›Œë“œ ì¶”ì¶œ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)"""
    STOPWORDS = {
        "ìˆ", "ì—†", "í•˜", "ë˜", "ì´", "ê·¸", "ì €", "ê²ƒ", "ìˆ˜", "ë“±", "ë°", "ì œ", "ì•½",
        "ì¦‰", "ë˜í•œ", "ë˜ëŠ”", "ì˜", "ê°€", "ì€", "ëŠ”", "ì„", "ë¥¼", "ì—", "ì—ì„œ"
    }
    
    all_words = []
    for comment in comments:
        clean = clean_text_for_bert(comment)
        words = [w for w in clean.split() if len(w) > 1 and w not in STOPWORDS]
        all_words.extend(words)
    
    word_counts = Counter(all_words)
    
    keywords = []
    for word, count in word_counts.most_common(top_k):
        keywords.append({
            "keyword": word,
            "count": count,
            "frequency": round(count / len(comments) * 100, 2)
        })
    
    return keywords
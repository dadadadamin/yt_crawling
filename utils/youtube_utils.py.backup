import os, time, httpx,csv
from dotenv import load_dotenv
from fastapi import HTTPException
from typing import List, Dict, Any, Optional
from collections import Counter
from statistics import mean

load_dotenv()
API_KEY = os.getenv("YT_API_KEY")
if not API_KEY:
    raise RuntimeError("환경변수 YT_API_KEY가 없습니다. (.env에 YT_API_KEY=YOUR_KEY 추가)")

SEARCH_URL = "https://www.googleapis.com/youtube/v3/search"
CHANNELS_URL = "https://www.googleapis.com/youtube/v3/channels"
VIDEOS_URL = "https://www.googleapis.com/youtube/v3/videos"
PLAYLIST_ITEMS_URL = "https://www.googleapis.com/youtube/v3/playlistItems"
COMMENT_THREADS_URL = "https://www.googleapis.com/youtube/v3/commentThreads"

def sleep_short():
    time.sleep(0.2)

def safe_get(url: str, params: dict) -> dict:
    with httpx.Client(timeout=20) as client:
        r = client.get(url, params=params)
        if r.status_code >= 400:
            raise HTTPException(status_code=r.status_code, detail=r.text)
        return r.json()

def fetch_channel_details(channel_ids: List[str], source_tag: str):
    from models.youtube_models import ChannelDetails
    rows: List[ChannelDetails] = []
    for i in range(0, len(channel_ids), 50):
        batch = channel_ids[i:i+50]
        params = {
            "part": "snippet,statistics,brandingSettings,topicDetails",
            "id": ",".join(batch),
            "key": API_KEY,
        }
        data = safe_get(CHANNELS_URL, params)
        for item in data.get("items", []):
            snippet = item.get("snippet", {}) or {}
            stats = item.get("statistics", {}) or {}
            branding = (item.get("brandingSettings", {}) or {}).get("channel", {}) or {}
            topics = (item.get("topicDetails", {}) or {}).get("topicCategories", []) or []
            thumbs = (snippet.get("thumbnails") or {})
            thumb_url = (thumbs.get("high") or thumbs.get("default") or {}).get("url")
            country = snippet.get("country") or branding.get("country")
            rows.append(ChannelDetails(
                channel_id=item.get("id"),
                title=snippet.get("title"),
                description=snippet.get("description"),
                custom_url=snippet.get("customUrl"),
                published_at=snippet.get("publishedAt"),
                country=country,
                subscriber_count=int(stats["subscriberCount"]) if "subscriberCount" in stats else None,
                video_count=int(stats["videoCount"]) if "videoCount" in stats else None,
                view_count=int(stats["viewCount"]) if "viewCount" in stats else None,
                topic_ids=topics or None,
                thumbnail_url=thumb_url,
                source=source_tag
            ))
        sleep_short()
    return rows

# 키워드 기반 채널 ID 검색
def search_channels_by_keyword(keyword: str, top_n: int, region: str = "KR", lang: str = "ko") -> List[str]:
    """
    키워드로 채널을 검색하여 ID 목록 반환 (Search: list API)
    """
    ids, page_token = [], None

    # API 호출 횟수를 줄이기 위해 maxResult 50 설정
    while len(ids) < top_n:
        params = {
            "part": "snippet",
            "type": "channel",
            "q": keyword,
            "maxResults": min(50, top_n - len(ids) + 20), # 넉넉하게 요청
            "key": API_KEY,
            "order": "relevance", # 관련도 순
            "regionCode": region,
            "relevanceLanguage": lang
        }
        if page_token:
            params["pageToken"] = page_token

        data = safe_get(SEARCH_URL, params)

        # 채널 ID만 추출
        for item in data.get("items", []):
            ch = item.get("id", {}).get("channelId")
            if ch and ch not in ids:
                ids.append(ch)
                if len(ids) >= top_n:
                    break # 목표한 10개를 채우면 중단

        page_token = data.get("nextPageToken")
        if not page_token:
            break # 다음 페이지가 없으면 중단
        sleep_short()
    
    return ids[:top_n] # 정확히 top_n 개수만큼 잘라서 반환


# 인기 영상 기반 채널 수집

def collect_channels_from_most_popular(region_code: str = "KR", pages: int = 5) -> List[str]:
    ch_ids = set()
    page_token, seen = None, 0
    while seen < pages:
        params = {"part": "snippet", "chart": "mostPopular", "regionCode": region_code,
                  "maxResults": 50, "key": API_KEY}
        if page_token: params["pageToken"] = page_token
        data = safe_get(VIDEOS_URL, params)
        for item in data.get("items", []):
            sn = item.get("snippet", {}) or {}
            if sn.get("channelId"): ch_ids.add(sn["channelId"])
        page_token = data.get("nextPageToken")
        seen += 1
        if not page_token: break
        sleep_short()
    return list(ch_ids)

def get_uploads_playlist_id(channel_id: str) -> Optional[str]:
    params = {"part": "contentDetails", "id": channel_id, "key": API_KEY}
    data = safe_get(CHANNELS_URL, params)
    items = data.get("items", [])
    if not items: return None
    return items[0].get("contentDetails", {}).get("relatedPlaylists", {}).get("uploads")

def get_recent_video_ids(uploads_playlist_id: str, max_results: int) -> List[str]:
    video_ids, page_token = [], None
    while len(video_ids) < max_results:
        params = {"part": "contentDetails", "playlistId": uploads_playlist_id,
                  "maxResults": min(50, max_results - len(video_ids)), "key": API_KEY}
        if page_token: params["pageToken"] = page_token
        data = safe_get(PLAYLIST_ITEMS_URL, params)
        for it in data.get("items", []):
            vid = it.get("contentDetails", {}).get("videoId")
            if vid: video_ids.append(vid)
        page_token = data.get("nextPageToken")
        if not page_token: break
        sleep_short()
    return video_ids

def get_video_stats(video_ids: List[str]):
    from models.youtube_models import VideoStatsOut
    rows: List[VideoStatsOut] = []
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i+50]
        params = {"part": "snippet,statistics", "id": ",".join(batch), "key": API_KEY}
        data = safe_get(VIDEOS_URL, params)
        for item in data.get("items", []):
            sn, st = item.get("snippet", {}) or {}, item.get("statistics", {}) or {}
            rows.append(VideoStatsOut(
                video_id=item.get("id"),
                video_title=sn.get("title"),
                video_published_at=sn.get("publishedAt"),
                view_count=int(st["viewCount"]) if "viewCount" in st else None,
                like_count=int(st["likeCount"]) if "likeCount" in st else None,
                comment_count=int(st["commentCount"]) if "commentCount" in st else None,
            ))
        sleep_short()
    return rows

def fetch_all_comments_for_video(video_id: str, include_replies: bool, max_total: int) -> List[str]:
    texts, fetched, page_token = [], 0, None
    while True:
        params = {"part": "snippet,replies", "videoId": video_id, "maxResults": 100,
                  "textFormat": "plainText", "key": API_KEY}
        if page_token: params["pageToken"] = page_token
        data = safe_get(COMMENT_THREADS_URL, params)
        for thread in data.get("items", []):
            top = thread.get("snippet", {}).get("topLevelComment", {}).get("snippet", {}) or {}
            if top.get("textDisplay"): texts.append(top["textDisplay"])
            fetched += 1
            if include_replies:
                replies = thread.get("replies", {}).get("comments", []) or []
                for rep in replies:
                    rs = rep.get("snippet", {}) or {}
                    if rs.get("textDisplay"): texts.append(rs["textDisplay"])
                    fetched += 1
            if fetched >= max_total: break
        if fetched >= max_total: break
        page_token = data.get("nextPageToken")
        if not page_token: break
        sleep_short()
    return texts

def extract_keywords_tfidf(texts: List[str], top_k: int) -> List[Dict[str, Any]]:
    tokens = " ".join(texts).split()
    common = Counter(tokens).most_common(top_k)
    return [{"keyword": k, "score": float(c), "method": "freq"} for k, c in common]

_POS = {"좋다", "감사", "최고", "재밌", "유익", "사랑", "추천", "대박", "멋지", "굿", "좋아요"}
_NEG = {"별로", "싫다", "최악", "지루", "짜증", "실망", "거짓", "광고", "시간낭비"}

def basic_sentiment_summary(texts: List[str]) -> Dict[str, Any]:
    pos = neg = neu = 0
    pos_ex, neg_ex = [], []
    for t in texts:
        if any(w in t for w in _POS):
            pos += 1
            if len(pos_ex) < 5: pos_ex.append(t[:80])
        elif any(w in t for w in _NEG):
            neg += 1
            if len(neg_ex) < 5: neg_ex.append(t[:80])
        else:
            neu += 1
    return {"positive": pos, "neutral": neu, "negative": neg,
            "examples": {"positive": pos_ex, "negative": neg_ex}}

# 채널의 최신 영상 통계 목록 '가져오기'
def get_recent_video_stats(channel_id: str, num_videos: int = 5):
    """
    채널의 최신 N개 영상 통계를 리스트로 반환 (API 호출)
    """
    from models.youtube_models import VideoStatsOut # 함수 내에서 import
    
    try:
        # 1. 채널의 업로드 플레이리스트 ID 가져오기
        playlist_id = get_uploads_playlist_id(channel_id)
        if not playlist_id:
            return []

        # 2. 최신 N개 영상 ID 가져오기
        video_ids = get_recent_video_ids(playlist_id, max_results=num_videos)
        if not video_ids:
            return []

        # 3. N개 영상의 통계 가져오기
        stats = get_video_stats(video_ids) # List[VideoStatsOut]
        return stats
    
    except Exception as e:
        print(f"[Error] get_recent_video_stats 실패 (Channel: {channel_id}): {e}")
        return []

# '가져온 통계'로 참여율 '계산하기'
def calculate_engagement_rate_from_stats(stats: list, subscriber_count: int) -> float | None:
    """
    (API 호출 없음) 영상 통계 리스트(stats)와 구독자 수로 참여율(%) 계산
    """
    if not subscriber_count or subscriber_count == 0:
        return None # 구독자 없으면 계산 불가
    if not stats:
        return 0.0

    # 평균 (좋아요 + 댓글 수) 계산
    total_likes = 0
    total_comments = 0
    for s in stats:
        total_likes += (s.like_count or 0)
        total_comments += (s.comment_count or 0)

    if len(stats) == 0:
        return 0.0

    avg_engagement = (total_likes + total_comments) / len(stats)
    
    # 참여율 = (평균 참여) / 구독자 수 * 100
    rate = (avg_engagement / subscriber_count) * 100
    return round(rate, 2) # 소수점 2자리까지

def compute_channel_engagement_rate(channel_id: str, max_videos: int = 8) -> Optional[float]:
    up = get_uploads_playlist_id(channel_id)
    if not up: return None
    vids = get_recent_video_ids(up, max_results=max_videos)
    if not vids: return None
    stats = get_video_stats(vids)
    rates = []
    for v in stats:
        views = v.view_count or 0
        likes = v.like_count or 0
        comments = v.comment_count or 0
        if views > 0:
            rates.append((likes + comments) / views * 100.0)
    return round(mean(rates), 2) if rates else None

def attach_metrics_to_channels(details: list) -> list:
    """
    ChannelDetails 리스트에 engagement_rate, roi(임시: subscriber_count)를 붙여
    ChannelWithMetrics로 변환해서 반환.
    """
    from models.youtube_models import ChannelWithMetrics
    out: list[ChannelWithMetrics] = []
    for ch in details:
        try:
            er = compute_channel_engagement_rate(ch.channel_id, max_videos=8)
        except Exception:
            er = None
        roi_placeholder = float(ch.subscriber_count or 0)  # 임시 ROI = 구독자수
        out.append(ChannelWithMetrics(
            **ch.model_dump(),
            engagement_rate=er,
            roi=roi_placeholder
        ))
    return out

# <최신 업로드 1개(가장 최근 영상) ID/제목 가져오기>
def get_latest_video_info(channel_id: str) -> Optional[Dict[str, str]]:
    """
    채널의 업로드 재생목록에서 '가장 최근' 영상 1개의 video_id와 제목을 반환
    """
    up = get_uploads_playlist_id(channel_id)
    if not up:
        return None

    # playlistItems는 최신이 먼저 오므로 maxResults=1
    params = {
        "part": "contentDetails",
        "playlistId": up,
        "maxResults": 1,
        "key": API_KEY,
    }
    data = safe_get(PLAYLIST_ITEMS_URL, params)
    items = data.get("items", [])
    if not items:
        return None
    vid = items[0].get("contentDetails", {}).get("videoId")
    if not vid:
        return None

    # 제목은 videos.list로 1회 조회
    vinfo = safe_get(VIDEOS_URL, {"part": "snippet", "id": vid, "key": API_KEY})
    title = None
    if vinfo.get("items"):
        title = (vinfo["items"][0].get("snippet") or {}).get("title")

    return {"video_id": vid, "video_title": title}


def save_comments_to_csv(rows: List[Dict[str, Any]], out_dir: str, base_name: str) -> str:
    """
    rows를 CSV로 저장. 파일 경로를 반환.
    """
    os.makedirs(out_dir, exist_ok=True)
    safe_name = "".join(ch if ch.isalnum() else "_" for ch in base_name)[:60]
    path = os.path.join(out_dir, f"{safe_name}.csv")

    # rows 예시 키: video_id, comment_id, parent_id, author, text, like_count, published_at
    fieldnames = ["video_id", "comment_id", "parent_id", "author", "text", "like_count", "published_at"]
    with open(path, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for r in rows:
            writer.writerow({
                "video_id": r.get("video_id"),
                "comment_id": r.get("comment_id"),
                "parent_id": r.get("parent_id"),
                "author": r.get("author"),
                "text": r.get("text"),
                "like_count": r.get("like_count"),
                "published_at": r.get("published_at"),
            })
    return path


def analyze_comments_keywords(texts: List[str], top_k: int = 40) -> List[Dict[str, Any]]:
    """
    간단 빈도 기반 키워드 추출. (한글/영문 공백 기준 토큰화)
    필요 시 불용어 처리/형태소 분석으로 고도화 가능.
    """
    # 너무 짧은/공백 문자열 제거
    cleaned = [t.strip() for t in texts if t and t.strip()]
    if not cleaned:
        return []

    tokens = " ".join(cleaned).split()
    from collections import Counter
    common = Counter(tokens).most_common(top_k)
    return [{"keyword": k, "score": float(c), "method": "freq"} for k, c in common]


def fetch_comments_structured_for_video(
    video_id: str,
    include_replies: bool = False,
    max_total: int = 500
) -> list[dict]:
    """
    commentThreads.list를 이용해
    - 특정 영상의 댓글(+선택적으로 대댓글)을 구조화해서 수집합니다.
    - 반환: [{video_id, comment_id, parent_id, author, text, like_count, published_at}, ...]
    """
    texts = []
    fetched = 0
    page_token = None

    while True:
        params = {
            "part": "snippet,replies",
            "videoId": video_id,
            "maxResults": 100,
            "textFormat": "plainText",
            "key": API_KEY,
        }
        if page_token:
            params["pageToken"] = page_token

        data = safe_get(COMMENT_THREADS_URL, params)

        for thread in data.get("items", []):
            # 상위 댓글
            top = thread.get("snippet", {}).get("topLevelComment", {}).get("snippet", {}) or {}
            comment_id = thread.get("id")
            texts.append({
                "video_id": video_id,
                "comment_id": comment_id,
                "parent_id": None,
                "author": top.get("authorDisplayName"),
                "text": top.get("textDisplay"),
                "like_count": top.get("likeCount"),
                "published_at": top.get("publishedAt"),
            })
            fetched += 1

            # 대댓글 수집 옵션
            if include_replies:
                replies = (thread.get("replies", {}) or {}).get("comments", []) or []
                for rep in replies:
                    r_snip = rep.get("snippet", {}) or {}
                    texts.append({
                        "video_id": video_id,
                        "comment_id": rep.get("id"),
                        "parent_id": comment_id,
                        "author": r_snip.get("authorDisplayName"),
                        "text": r_snip.get("textDisplay"),
                        "like_count": r_snip.get("likeCount"),
                        "published_at": r_snip.get("publishedAt"),
                    })
                    fetched += 1

            if fetched >= max_total:
                break

        if fetched >= max_total:
            break

        page_token = data.get("nextPageToken")
        if not page_token:
            break

        sleep_short()

    return texts

#현재 테스트용 
#초기화면 유튜버 구성 구독자 수 100만 이상 유튜버들 나열 
def filter_large_channels(channels: list, min_subs: int = 1000000):
    """
    구독자 수 기준으로 필터링.
    channels: ChannelDetails 리스트
    min_subs: 최소 구독자 수 (기본 100만)
    """
    return [
        ch for ch in channels
        if ch.subscriber_count is not None and ch.subscriber_count >= min_subs
    ]
#기업 유튜버 제외 시나리오
def is_personnal_channel(ch)->bool:
    '''휴리스틱 필터
    ch:ChennelDetails
    '''
    title=(ch.title or"").lower()
    desc=(ch.description or"").lower()

    #키워드 기반 제외
    corp_kw=[
        "official","channel","music","news","entertainment", "company", "corporation","record",
        "group","media","press","공식","뉴스","엔터","방송","레코드","기획사","agency","jyp","yg","sm","hybe","cj"
    ]
    if any(kw in title for kw in corp_kw):
        return False
    # fix: check keyword presence in description (was 'desc in title')
    if any(kw in desc for kw in corp_kw):
        return False
    
    #영상수로 제외 (영상 1만개당 구독자 100만이면 제외)
    if ch.subscriber_count and ch.video_count:
        ratio=ch.video_count/ch.subscriber_count #영상 수/구독자 수
        if ratio>0.01:
            return False
        
    #토픽 기반 제외
    if ch.topic_ids:
        joined=",".join(ch.topic_ids).lower()
        if any(k in joined for k in["music","tv","corporation"]):
            return False
    #국가 필터
    if ch.country and ch.country and ch.country!="KR":
        return False
    
    return True


# -----------------------------
# CSV Export helpers
# -----------------------------
def export_influencer_metadata_csv(
    channel_ids: List[str],
    out_path: str,
    max_videos_for_avg: int = 5
) -> str:
    """
    채널 메타데이터 CSV 생성
    컬럼: channel_id, title, subscriber_count, view_count, video_count,
          avg_like_count, avg_comment_count, engagement_rate, thumbnail_url
    """
    os.makedirs(os.path.dirname(out_path) or ".", exist_ok=True)

    details = fetch_channel_details(channel_ids, source_tag="export:meta")

    fieldnames = [
        "channel_id", "title",
        "subscriber_count", "view_count", "video_count",
        "avg_like_count", "avg_comment_count",
        "engagement_rate", "thumbnail_url"
    ]
    with open(out_path, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for ch in details:
            # 최근 영상 기반 평균 like/comment 및 참여율
            stats = get_recent_video_stats(ch.channel_id, num_videos=max_videos_for_avg)
            if stats:
                avg_like = round(sum((s.like_count or 0) for s in stats) / len(stats), 2)
                avg_comment = round(sum((s.comment_count or 0) for s in stats) / len(stats), 2)
            else:
                avg_like = 0.0
                avg_comment = 0.0
            eng_rate = calculate_engagement_rate_from_stats(stats, ch.subscriber_count or 0)

            writer.writerow({
                "channel_id": ch.channel_id,
                "title": ch.title,
                "subscriber_count": ch.subscriber_count,
                "view_count": ch.view_count,
                "video_count": ch.video_count,
                "avg_like_count": avg_like,
                "avg_comment_count": avg_comment,
                "engagement_rate": eng_rate,
                "thumbnail_url": ch.thumbnail_url,
            })
    return out_path


def export_video_info_csv(
    video_ids: List[str],
    out_path: str
) -> str:
    """
    영상 정보 CSV 생성
    컬럼: video_id, title, description, transcript, tags, thumbnail_url
    """
    os.makedirs(os.path.dirname(out_path) or ".", exist_ok=True)

    rows: List[Dict[str, Any]] = []
    # videos.list는 최대 50개씩
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i+50]
        data = safe_get(VIDEOS_URL, {"part": "snippet", "id": ",".join(batch), "key": API_KEY})
        for item in data.get("items", []):
            vid = item.get("id")
            sn = item.get("snippet", {}) or {}
            title = sn.get("title", "")
            desc = sn.get("description", "")
            tags = sn.get("tags", []) or []
            thumbs = sn.get("thumbnails", {}) or {}
            thumb = (thumbs.get("high") or thumbs.get("default") or {}).get("url", "")

            # transcript (best-effort)
            transcript_text = ""
            try:
                from youtube_transcript_api import YouTubeTranscriptApi
                transcript_list = YouTubeTranscriptApi.get_transcript(vid, languages=["ko", "en"])
                transcript_text = " ".join(seg["text"] for seg in transcript_list)
            except Exception:
                transcript_text = ""

            rows.append({
                "video_id": vid,
                "title": title,
                "description": desc,
                "transcript": transcript_text,
                "tags": ", ".join(tags),
                "thumbnail_url": thumb
            })
        sleep_short()

    fieldnames = ["video_id", "title", "description", "transcript", "tags", "thumbnail_url"]
    with open(out_path, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for r in rows:
            writer.writerow(r)
    return out_path


# -----------------------------
# Category-based Influencer Export
# -----------------------------
def _pick_top_channels_for_category(category_keyword: str, target_count: int = 10) -> List[str]:
    """
    키워드 기반으로 채널을 넉넉히 검색한 뒤,
    - 개인 채널 휴리스틱 필터
    - 구독자 10만~100만
    을 적용해 최대 target_count개 채널 ID를 골라 반환
    """
    # 넉넉히 60개 검색 후 필터
    candidates = search_channels_by_keyword(category_keyword, top_n=max(60, target_count * 6))
    if not candidates:
        return []

    details = fetch_channel_details(candidates, source_tag=f"category:{category_keyword}")
    picked: List[str] = []
    for ch in details:
        subs = ch.subscriber_count or 0
        if 100000 <= subs <= 1000000 and is_personnal_channel(ch):
            picked.append(ch.channel_id)
        if len(picked) >= target_count:
            break
    return picked


def export_category_influencers_csv(
    categories: List[str],
    out_path: str,
    recent_videos_for_sum: int = 20
) -> str:
    """
    카테고리별로 채널 최대 10개를 추출하여 CSV 저장
    컬럼:
      category, channel_id, title, description, subscriber_count,
      channel_total_views, recent_uploads_views_sum, country, thumbnail_url
    """
    os.makedirs(os.path.dirname(out_path) or ".", exist_ok=True)

    fieldnames = [
        "category", "channel_id", "title", "description",
        "subscriber_count", "channel_total_views",
        "recent_uploads_views_sum", "country", "thumbnail_url"
    ]

    with open(out_path, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        for cat in list(dict.fromkeys(categories)):
            try:
                channel_ids = _pick_top_channels_for_category(cat, target_count=10)
                if not channel_ids:
                    continue
                infos = fetch_channel_details(channel_ids, source_tag=f"category:{cat}")

                for ch in infos:
                    # 최근 업로드 합계 조회수 (quota 절약을 위해 최근 N개 한정)
                    uploads_id = get_uploads_playlist_id(ch.channel_id)
                    recent_ids = get_recent_video_ids(uploads_id, max_results=recent_videos_for_sum) if uploads_id else []
                    stats = get_video_stats(recent_ids) if recent_ids else []
                    recent_views_sum = int(sum((s.view_count or 0) for s in stats)) if stats else 0

                    writer.writerow({
                        "category": cat,
                        "channel_id": ch.channel_id,
                        "title": ch.title,
                        "description": ch.description,
                        "subscriber_count": ch.subscriber_count,
                        "channel_total_views": ch.view_count,
                        "recent_uploads_views_sum": recent_views_sum,
                        "country": ch.country,
                        "thumbnail_url": ch.thumbnail_url
                    })
            except Exception:
                # 카테고리 단위에서 오류가 나더라도 나머지 진행
                continue

    return out_path


# -----------------------------
# Channel latest videos (+ top comments) export
# -----------------------------
def _fetch_video_snippets(video_ids: List[str]) -> Dict[str, Dict[str, Any]]:
    """
    videos.list 로 title/description/tags/thumbnail 수집
    반환: {video_id: {...}}
    """
    out: Dict[str, Dict[str, Any]] = {}
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i+50]
        data = safe_get(VIDEOS_URL, {"part": "snippet", "id": ",".join(batch), "key": API_KEY})
        for item in data.get("items", []):
            vid = item.get("id")
            sn = item.get("snippet", {}) or {}
            thumbs = sn.get("thumbnails", {}) or {}
            thumb = (thumbs.get("high") or thumbs.get("default") or {}).get("url", "")
            out[vid] = {
                "title": sn.get("title", ""),
                "description": sn.get("description", ""),
                "tags": ", ".join(sn.get("tags", []) or []),
                "thumbnail_url": thumb,
            }
        sleep_short()
    return out


def _fetch_transcript_text(video_id: str) -> str:
    try:
        from youtube_transcript_api import YouTubeTranscriptApi
        tr = YouTubeTranscriptApi.get_transcript(video_id, languages=["ko", "en"])
        return " ".join(seg["text"] for seg in tr)
    except Exception:
        return ""


def fetch_top_comments_for_video(
    video_id: str,
    top_n: int = 10,
    max_scan: int = 300
) -> List[Dict[str, Any]]:
    """
    좋아요 수 기준 상위 N개 댓글 반환 (최대 max_scan개 스캔)
    반환 항목: comment_id, text, like_count, author, published_at
    """
    collected: List[Dict[str, Any]] = []
    scanned = 0
    page_token = None
    while True and scanned < max_scan:
        params = {
            "part": "snippet",
            "videoId": video_id,
            "maxResults": 100,
            "textFormat": "plainText",
            "key": API_KEY,
        }
        if page_token:
            params["pageToken"] = page_token

        data = safe_get(COMMENT_THREADS_URL, params)
        for thread in data.get("items", []):
            sn = (thread.get("snippet", {}) or {}).get("topLevelComment", {}) or {}
            top_sn = sn.get("snippet", {}) or {}
            collected.append({
                "comment_id": (thread.get("id") or ""),
                "text": top_sn.get("textDisplay"),
                "like_count": int(top_sn.get("likeCount") or 0),
                "author": top_sn.get("authorDisplayName"),
                "published_at": top_sn.get("publishedAt"),
            })
            scanned += 1
            if scanned >= max_scan:
                break
        if scanned >= max_scan:
            break
        page_token = data.get("nextPageToken")
        if not page_token:
            break
        sleep_short()

    collected.sort(key=lambda x: x.get("like_count", 0), reverse=True)
    return collected[:top_n]


def export_channel_latest_videos_with_comments_csv(
    channel_id: str,
    out_path: str,
    num_videos: int = 3,
    top_comments: int = 10
) -> str:
    """
    채널의 최근 num_videos개 영상에 대해
    - video_id, title, description, transcript, tags, thumbnail_url
    - 좋아요 순 상위 top_comments 댓글
    을 CSV로 저장

    CSV 칼럼:
      video_id, title, description, transcript, tags, thumbnail_url,
      comment_rank, comment_id, comment_text, comment_like_count, comment_author, comment_published_at
    """
    os.makedirs(os.path.dirname(out_path) or ".", exist_ok=True)

    uploads = get_uploads_playlist_id(channel_id)
    if not uploads:
        raise HTTPException(status_code=404, detail="업로드 재생목록을 찾을 수 없습니다.")
    video_ids = get_recent_video_ids(uploads, max_results=num_videos)
    if not video_ids:
        raise HTTPException(status_code=404, detail="최근 영상을 찾을 수 없습니다.")

    snippets = _fetch_video_snippets(video_ids)

    fieldnames = [
        "video_id", "title", "description", "transcript", "tags", "thumbnail_url",
        "comment_rank", "comment_id", "comment_text", "comment_like_count",
        "comment_author", "comment_published_at"
    ]

    with open(out_path, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        for vid in video_ids:
            meta = snippets.get(vid, {})
            transcript = _fetch_transcript_text(vid)
            top_comms = fetch_top_comments_for_video(vid, top_n=top_comments, max_scan=300)

            if not top_comms:
                writer.writerow({
                    "video_id": vid,
                    "title": meta.get("title", ""),
                    "description": meta.get("description", ""),
                    "transcript": transcript,
                    "tags": meta.get("tags", ""),
                    "thumbnail_url": meta.get("thumbnail_url", ""),
                    "comment_rank": None,
                    "comment_id": None,
                    "comment_text": None,
                    "comment_like_count": None,
                    "comment_author": None,
                    "comment_published_at": None,
                })
                continue

            for rank, c in enumerate(top_comms, start=1):
                writer.writerow({
                    "video_id": vid,
                    "title": meta.get("title", ""),
                    "description": meta.get("description", ""),
                    "transcript": transcript,
                    "tags": meta.get("tags", ""),
                    "thumbnail_url": meta.get("thumbnail_url", ""),
                    "comment_rank": rank,
                    "comment_id": c.get("comment_id"),
                    "comment_text": c.get("text"),
                    "comment_like_count": c.get("like_count"),
                    "comment_author": c.get("author"),
                    "comment_published_at": c.get("published_at"),
                })

    return out_path
